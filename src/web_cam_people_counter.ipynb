{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OpenCV to get frames from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# HLS playlist URL\n",
    "hls_url = \"https://streamer4.brownrice.com/camdensnowbowl1/camdensnowbowl1.stream/main_playlist.m3u8\"\n",
    "\n",
    "cap = cv2.VideoCapture(hls_url)\n",
    "\n",
    "frame_count = 0\n",
    "frame_skip = 100  # Skip every 5 frames\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the HLS stream.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Frame not received, ending stream\")\n",
    "            break\n",
    "        frame_count += 1\n",
    "        if frame_count % frame_skip != 0:\n",
    "            continue\n",
    "        # Process your frame (e.g., people counting) here\n",
    "\n",
    "        # For debugging, display the frame\n",
    "        cv2.imshow(\"HLS Stream\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count People in Web Cam Using YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Import the necessary classes - this is a new issue with pytorch 2.6, could use 2.4 and not have to import all of these layers and add them to the globals list\n",
    "from ultralytics.nn.tasks import DetectionModel  # Already imported for YOLOv8 models\n",
    "from torch.nn.modules.container import Sequential    # For Sequential layers\n",
    "from ultralytics.nn.modules.conv import Conv         # For Conv layers defined by Ultralytics\n",
    "from torch.nn.modules.conv import Conv2d              # For PyTorch's Conv2d layer\n",
    "from torch.nn.modules.batchnorm import BatchNorm2d\n",
    "from torch.nn.modules.activation import SiLU               # PyTorch's SiLU activation\n",
    "from ultralytics.nn.modules.block import C2f                       # Ultralytics' C2f block\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from ultralytics.nn.modules.block import Bottleneck\n",
    "from ultralytics.nn.modules.block import SPPF\n",
    "from torch.nn.modules.pooling import MaxPool2d\n",
    "from torch.nn.modules.upsampling import Upsample\n",
    "from ultralytics.nn.modules.conv import Concat\n",
    "from ultralytics.nn.modules.head import Detect\n",
    "from ultralytics.nn.modules.block import DFL\n",
    "torch.serialization.add_safe_globals([\n",
    "    DetectionModel, Sequential, Conv, Conv2d, BatchNorm2d, SiLU, C2f, ModuleList,\n",
    "    Bottleneck, SPPF, MaxPool2d, Upsample, Concat, Detect, DFL\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the pretrained YOLOv8 model and move it to the appropriate device\n",
    "model = YOLO(\"yolov8n.pt\").to(device)\n",
    "\n",
    "# Define the HLS stream URL (the direct stream URL you extracted)\n",
    "hls_url = \"https://streamer4.brownrice.com/camdensnowbowl1/camdensnowbowl1.stream/main_playlist.m3u8\"\n",
    "\n",
    "# Open the video stream\n",
    "cap = cv2.VideoCapture(hls_url)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the video stream.\")\n",
    "    exit()\n",
    "\n",
    "frame_skip = 20  # Process every 20th frame\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "    frame_count += 1\n",
    "\n",
    "    # Skip frames until we hit the desired interval\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue\n",
    "\n",
    "    # Run YOLOv8 inference on the current frame.\n",
    "    results = model(frame)\n",
    "    \n",
    "    # We'll use a copy of the frame to draw annotations.\n",
    "    annotated_frame = frame.copy()\n",
    "    people_count = 0\n",
    "\n",
    "    # Process each detection result\n",
    "    for result in results:\n",
    "        if result.boxes is not None:\n",
    "            boxes = result.boxes.data.cpu().numpy()  # shape: (num_boxes, 6)\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2, conf, cls = box\n",
    "                # In COCO, the 'person' class typically has an id of 0.\n",
    "                if int(cls) >= 0:\n",
    "                    bbox_color = (0, 255, 0)  # Green\n",
    "                    if cls != 0:\n",
    "                        bbox_color = (255, 0, 0)\n",
    "                    cv2.rectangle(annotated_frame, (int(x1), int(y1)), (int(x2), int(y2)), bbox_color, 2)\n",
    "                    people_count += 1\n",
    "                    # Draw the bounding box and label on the frame.\n",
    "                    cv2.putText(\n",
    "                        annotated_frame,\n",
    "                        f\"{int(cls)} {conf:.2f}\",\n",
    "                        (int(x1), int(y1) - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.5,\n",
    "                        (0, 255, 0),\n",
    "                        2,\n",
    "                    )\n",
    "    \n",
    "    # Overlay the people count on the frame.\n",
    "    cv2.putText(\n",
    "        annotated_frame,\n",
    "        f\"People Count: {people_count}\",\n",
    "        (10, 30),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (0, 0, 255),\n",
    "        2,\n",
    "    )\n",
    "    \n",
    "    # Display the annotated frame\n",
    "    cv2.imshow(\"YOLOv8 People Counting\", annotated_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the stream and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset we can use to fine tune the model for counting people on this web cam\n",
    "Saves every 1000th frame into the dataset/images/train folder and the associated YOLOv8 detected people annotations into the dataset/labels/train folder\n",
    "\n",
    "Idea is we then go through these images after we gather a lot and improve upon the annotations. Then we fine tune the YOLO model with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# Set up safe globals for PyTorch 2.6+ (include only if necessary)\n",
    "from ultralytics.nn.tasks import DetectionModel\n",
    "from torch.nn.modules.container import Sequential, ModuleList\n",
    "from ultralytics.nn.modules.conv import Conv, Concat\n",
    "from torch.nn.modules.conv import Conv2d\n",
    "from torch.nn.modules.batchnorm import BatchNorm2d\n",
    "from torch.nn.modules.activation import SiLU\n",
    "from ultralytics.nn.modules.block import C2f, Bottleneck, SPPF, DFL\n",
    "from torch.nn.modules.pooling import MaxPool2d\n",
    "from torch.nn.modules.upsampling import Upsample\n",
    "from ultralytics.nn.modules.head import Detect\n",
    "torch.serialization.add_safe_globals([\n",
    "    DetectionModel, Sequential, Conv, Conv2d, BatchNorm2d, SiLU, C2f, ModuleList,\n",
    "    Bottleneck, SPPF, MaxPool2d, Upsample, Concat, Detect, DFL\n",
    "])\n",
    "\n",
    "# Adjustable parameters\n",
    "WEBCAM_URL = \"https://streamer4.brownrice.com/camdensnowbowl1/camdensnowbowl1.stream/main_playlist.m3u8\"\n",
    "FRAME_INTERVAL = 1000  # Process every 1000th frame\n",
    "CONF_THRESHOLD = 0.4   # Confidence threshold\n",
    "IMG_DIR = 'dataset/images/train'\n",
    "LABEL_DIR = 'dataset/labels/train'\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\").to(device)\n",
    "\n",
    "cap = cv2.VideoCapture(WEBCAM_URL)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the video stream.\")\n",
    "    exit()\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    # Process only every FRAME_INTERVAL frame\n",
    "    if frame_count % FRAME_INTERVAL != 1:\n",
    "        continue\n",
    "\n",
    "    # Save the raw frame\n",
    "    timestamp = int(time.time())\n",
    "    img_filename = os.path.join(IMG_DIR, f\"frame_{frame_count}_{timestamp}.jpg\")\n",
    "    cv2.imwrite(img_filename, frame)\n",
    "    print(f\"Saved image: {img_filename}\")\n",
    "\n",
    "    # Run YOLOv8 inference on the frame\n",
    "    results = model(frame)\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # Open a .txt file for writing the annotations in YOLO format\n",
    "    txt_filename = os.path.splitext(img_filename)[0] + \".txt\"\n",
    "    txt_filename = txt_filename.replace(\"images\", \"labels\")\n",
    "    with open(txt_filename, \"w\") as f:\n",
    "        for result in results:\n",
    "            if result.boxes is not None:\n",
    "                boxes = result.boxes.data.cpu().numpy()  # each row: [x1, y1, x2, y2, conf, cls]\n",
    "                for box in boxes:\n",
    "                    x1, y1, x2, y2, conf, cls = box\n",
    "                    if conf >= CONF_THRESHOLD and int(cls) == 0:  # Only person (class 0)\n",
    "                        # Convert bounding box to YOLO format (normalized)\n",
    "                        x_center = ((x1 + x2) / 2.0) / width\n",
    "                        y_center = ((y1 + y2) / 2.0) / height\n",
    "                        bbox_width = (x2 - x1) / width\n",
    "                        bbox_height = (y2 - y1) / height\n",
    "                        # Write annotation line: class x_center y_center width height\n",
    "                        f.write(f\"0 {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\\n\")\n",
    "    print(f\"Saved annotations: {txt_filename}\")\n",
    "\n",
    "    # Optionally, display the frame (with no annotations drawn)\n",
    "    #cv2.imshow(\"Dataset Collection\", frame)\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025_02_10_11_56_04_858392'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n",
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "people_counter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
