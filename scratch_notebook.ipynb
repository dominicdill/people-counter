{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OpenCV to get frames from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# HLS playlist URL\n",
    "hls_url = \"https://streamer4.brownrice.com/camdensnowbowl1/camdensnowbowl1.stream/main_playlist.m3u8\"\n",
    "\n",
    "cap = cv2.VideoCapture(hls_url)\n",
    "\n",
    "frame_count = 0\n",
    "frame_skip = 100  # Skip every 5 frames\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the HLS stream.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Frame not received, ending stream\")\n",
    "            break\n",
    "        frame_count += 1\n",
    "        if frame_count % frame_skip != 0:\n",
    "            continue\n",
    "        # Process your frame (e.g., people counting) here\n",
    "\n",
    "        # For debugging, display the frame\n",
    "        cv2.imshow(\"HLS Stream\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count People in Web Cam Using YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Import the necessary classes - this is a new issue with pytorch 2.6, could use 2.4 and not have to import all of these layers and add them to the globals list\n",
    "from ultralytics.nn.tasks import DetectionModel  # Already imported for YOLOv8 models\n",
    "from torch.nn.modules.container import Sequential    # For Sequential layers\n",
    "from ultralytics.nn.modules.conv import Conv         # For Conv layers defined by Ultralytics\n",
    "from torch.nn.modules.conv import Conv2d              # For PyTorch's Conv2d layer\n",
    "from torch.nn.modules.batchnorm import BatchNorm2d\n",
    "from torch.nn.modules.activation import SiLU               # PyTorch's SiLU activation\n",
    "from ultralytics.nn.modules.block import C2f                       # Ultralytics' C2f block\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from ultralytics.nn.modules.block import Bottleneck\n",
    "from ultralytics.nn.modules.block import SPPF\n",
    "from torch.nn.modules.pooling import MaxPool2d\n",
    "from torch.nn.modules.upsampling import Upsample\n",
    "from ultralytics.nn.modules.conv import Concat\n",
    "from ultralytics.nn.modules.head import Detect\n",
    "from ultralytics.nn.modules.block import DFL\n",
    "torch.serialization.add_safe_globals([\n",
    "    DetectionModel, Sequential, Conv, Conv2d, BatchNorm2d, SiLU, C2f, ModuleList,\n",
    "    Bottleneck, SPPF, MaxPool2d, Upsample, Concat, Detect, DFL\n",
    "])\n",
    "\n",
    "from src.config.config import frame_capture_settings\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the pretrained YOLOv8 model and move it to the appropriate device\n",
    "model = YOLO(os.path.join(frame_capture_settings.model_path, frame_capture_settings.model_name)).to(device)\n",
    "print(model.names)\n",
    "\n",
    "# Define the HLS stream URL (the direct stream URL you extracted)\n",
    "hls_url = \"https://streamer4.brownrice.com/camdensnowbowl1/camdensnowbowl1.stream/main_playlist.m3u8\"\n",
    "\n",
    "# Open the video stream\n",
    "cap = cv2.VideoCapture(hls_url)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the video stream.\")\n",
    "    exit()\n",
    "\n",
    "frame_skip = 20  # Process every 20th frame\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "    frame_count += 1\n",
    "\n",
    "    # Skip frames until we hit the desired interval\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue\n",
    "\n",
    "    # Run YOLOv8 inference on the current frame.\n",
    "    results = model(frame)\n",
    "    \n",
    "    # We'll use a copy of the frame to draw annotations.\n",
    "    annotated_frame = frame.copy()\n",
    "    people_count = 0\n",
    "\n",
    "    # Process each detection result\n",
    "    for result in results:\n",
    "        if result.boxes is not None:\n",
    "            boxes = result.boxes.data.cpu().numpy()  # shape: (num_boxes, 6)\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2, conf, cls = box\n",
    "                # In COCO, the 'person' class typically has an id of 0.\n",
    "                if int(cls) >= 0:\n",
    "                    bbox_color = (0, 255, 0)  # Green\n",
    "                    if cls != 0:\n",
    "                        bbox_color = (255, 0, 0)\n",
    "                    cv2.rectangle(annotated_frame, (int(x1), int(y1)), (int(x2), int(y2)), bbox_color, 2)\n",
    "                    people_count += 1\n",
    "                    # Draw the bounding box and label on the frame.\n",
    "                    cv2.putText(\n",
    "                        annotated_frame,\n",
    "                        f\"{int(cls)} {conf:.2f}\",\n",
    "                        (int(x1), int(y1) - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.5,\n",
    "                        (0, 255, 0),\n",
    "                        2,\n",
    "                    )\n",
    "    \n",
    "    # Overlay the people count on the frame.\n",
    "    cv2.putText(\n",
    "        annotated_frame,\n",
    "        f\"People Count: {people_count}\",\n",
    "        (10, 30),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (0, 0, 255),\n",
    "        2,\n",
    "    )\n",
    "    \n",
    "    # Display the annotated frame\n",
    "    cv2.imshow(\"YOLOv8 People Counting\", annotated_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the stream and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset we can use to fine tune the model for counting people on this web cam\n",
    "Saves every 1000th frame into the dataset/images/train folder and the associated YOLOv8 detected people annotations into the dataset/labels/train folder\n",
    "\n",
    "Idea is we then go through these images after we gather a lot and improve upon the annotations. Then we fine tune the YOLO model with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# Set up safe globals for PyTorch 2.6+ (include only if necessary)\n",
    "from ultralytics.nn.tasks import DetectionModel\n",
    "from torch.nn.modules.container import Sequential, ModuleList\n",
    "from ultralytics.nn.modules.conv import Conv, Concat\n",
    "from torch.nn.modules.conv import Conv2d\n",
    "from torch.nn.modules.batchnorm import BatchNorm2d\n",
    "from torch.nn.modules.activation import SiLU\n",
    "from ultralytics.nn.modules.block import C2f, Bottleneck, SPPF, DFL\n",
    "from torch.nn.modules.pooling import MaxPool2d\n",
    "from torch.nn.modules.upsampling import Upsample\n",
    "from ultralytics.nn.modules.head import Detect\n",
    "torch.serialization.add_safe_globals([\n",
    "    DetectionModel, Sequential, Conv, Conv2d, BatchNorm2d, SiLU, C2f, ModuleList,\n",
    "    Bottleneck, SPPF, MaxPool2d, Upsample, Concat, Detect, DFL\n",
    "])\n",
    "\n",
    "# Adjustable parameters\n",
    "WEBCAM_URL = \"https://streamer4.brownrice.com/camdensnowbowl1/camdensnowbowl1.stream/main_playlist.m3u8\"\n",
    "FRAME_INTERVAL = 1000  # Process every 1000th frame\n",
    "CONF_THRESHOLD = 0.4   # Confidence threshold\n",
    "IMG_DIR = 'dataset/images/train'\n",
    "LABEL_DIR = 'dataset/labels/train'\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\").to(device)\n",
    "\n",
    "cap = cv2.VideoCapture(WEBCAM_URL)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the video stream.\")\n",
    "    exit()\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    # Process only every FRAME_INTERVAL frame\n",
    "    if frame_count % FRAME_INTERVAL != 1:\n",
    "        continue\n",
    "\n",
    "    # Save the raw frame\n",
    "    timestamp = int(time.time())\n",
    "    img_filename = os.path.join(IMG_DIR, f\"frame_{frame_count}_{timestamp}.jpg\")\n",
    "    cv2.imwrite(img_filename, frame)\n",
    "    print(f\"Saved image: {img_filename}\")\n",
    "\n",
    "    # Run YOLOv8 inference on the frame\n",
    "    results = model(frame)\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # Open a .txt file for writing the annotations in YOLO format\n",
    "    txt_filename = os.path.splitext(img_filename)[0] + \".txt\"\n",
    "    txt_filename = txt_filename.replace(\"images\", \"labels\")\n",
    "    with open(txt_filename, \"w\") as f:\n",
    "        for result in results:\n",
    "            if result.boxes is not None:\n",
    "                boxes = result.boxes.data.cpu().numpy()  # each row: [x1, y1, x2, y2, conf, cls]\n",
    "                for box in boxes:\n",
    "                    x1, y1, x2, y2, conf, cls = box\n",
    "                    if conf >= CONF_THRESHOLD and int(cls) == 0:  # Only person (class 0)\n",
    "                        # Convert bounding box to YOLO format (normalized)\n",
    "                        x_center = ((x1 + x2) / 2.0) / width\n",
    "                        y_center = ((y1 + y2) / 2.0) / height\n",
    "                        bbox_width = (x2 - x1) / width\n",
    "                        bbox_height = (y2 - y1) / height\n",
    "                        # Write annotation line: class x_center y_center width height\n",
    "                        f.write(f\"0 {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\\n\")\n",
    "    print(f\"Saved annotations: {txt_filename}\")\n",
    "\n",
    "    # Optionally, display the frame (with no annotations drawn)\n",
    "    #cv2.imshow(\"Dataset Collection\", frame)\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "# Hardcoded folder path (change this to your folder)\n",
    "folder_path = 'dataset/labels/model_default'\n",
    "\n",
    "# Define EST timezone (fixed offset of -5 hours from UTC)\n",
    "est_tz = timezone(timedelta(hours=-5))\n",
    "\n",
    "# Loop over all items in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Process only files starting with \"frame\"\n",
    "    if filename.startswith(\"frame\"):\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "        if not os.path.isfile(full_path):\n",
    "            continue  # Skip if not a file\n",
    "\n",
    "        # Separate the file name from its extension\n",
    "        base, ext = os.path.splitext(filename)\n",
    "        parts = base.split(\"_\")\n",
    "        \n",
    "        # We expect at least three parts (e.g., \"frame\", \"146001\", \"1738885302\")\n",
    "        if len(parts) < 3:\n",
    "            print(f\"Skipping {filename}: does not have enough underscore-separated parts.\")\n",
    "            continue\n",
    "\n",
    "        # The last part should be a Unix timestamp (as a string)\n",
    "        timestamp_str = parts[-1]\n",
    "        try:\n",
    "            timestamp_val = int(timestamp_str)\n",
    "        except ValueError:\n",
    "            print(f\"Skipping {filename}: timestamp '{timestamp_str}' is not an integer.\")\n",
    "            continue\n",
    "\n",
    "        # Convert the Unix timestamp to a datetime in EST\n",
    "        dt = datetime.fromtimestamp(timestamp_val, tz=est_tz)\n",
    "        formatted_dt = dt.strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n",
    "\n",
    "        # Build the new file name\n",
    "        new_filename = f\"camdensnowbowl_{formatted_dt}{ext}\"\n",
    "        new_full_path = os.path.join(folder_path, new_filename)\n",
    "\n",
    "        # Print the rename operation and perform the renaming\n",
    "        print(f\"Renaming '{filename}' to '{new_filename}'\")\n",
    "        os.rename(full_path, new_full_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/finetune_model_training.py\n",
    "\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from config.config import frame_capture_settings  # Adjust if necessary\n",
    "\n",
    "\n",
    "def fine_tune_external():\n",
    "    \"\"\"\n",
    "    Stage 1: Fine tune on a large, external people detection dataset.\n",
    "    The external dataset should be specified in dataset/data_external.yaml.\n",
    "    \"\"\"\n",
    "    print(\"Starting fine tuning on external dataset...\")\n",
    "\n",
    "    # Path to your pre-trained model (e.g., YOLOv8n)\n",
    "    model_path = os.path.join(frame_capture_settings.model_path, frame_capture_settings.model_name)\n",
    "    model = YOLO(model_path)\n",
    "\n",
    "    # YAML configuration for external dataset (ensure this file exists and is configured properly)\n",
    "    external_data_yaml = \"dataset/data_external.yaml\"\n",
    "\n",
    "    # Fine tune on the external dataset\n",
    "    results = model.train(\n",
    "        data=external_data_yaml,       # external dataset configuration\n",
    "        epochs=50,                     # adjust epochs as needed\n",
    "        imgsz=640,                     # target image size\n",
    "        batch=16,                      # batch size (adjust based on your hardware)\n",
    "        project=\"finetune\",            # project name for saving results\n",
    "        name=\"yolov8n_external_finetune\"  # run name for external fine tuning\n",
    "    )\n",
    "\n",
    "    print(\"External fine tuning complete!\")\n",
    "    # Optionally, you can save the model checkpoint explicitly.\n",
    "    # For example:\n",
    "    # model.save(\"src/models/yolov8n_external_finetune.pt\")\n",
    "    # Here, we assume that the best checkpoint is saved by default (e.g., in runs/train/yolov8n_external_finetune/weights/)\n",
    "    return model  # returning the fine tuned model\n",
    "\n",
    "\n",
    "def fine_tune_target(model_checkpoint=None):\n",
    "    \"\"\"\n",
    "    Stage 2: Fine tune on the target domain (ski resort webcam images).\n",
    "    The target domain dataset should be specified in dataset/data_target.yaml.\n",
    "    If a checkpoint is provided, load that; otherwise, use the default best checkpoint from stage 1.\n",
    "    \"\"\"\n",
    "    print(\"Starting fine tuning on target domain...\")\n",
    "\n",
    "    # Load the model from a checkpoint if provided; otherwise, assume the external fine tuning best checkpoint is in a known location.\n",
    "    if model_checkpoint:\n",
    "        model = YOLO(model_checkpoint)\n",
    "    else:\n",
    "        # Adjust this path if necessary based on where the external fine tuning checkpoint is saved.\n",
    "        checkpoint_path = \"runs/train/yolov8n_external_finetune/weights/best.pt\"\n",
    "        model = YOLO(checkpoint_path)\n",
    "\n",
    "    # YAML configuration for target domain dataset (ski resort images)\n",
    "    target_data_yaml = \"dataset/data_target.yaml\"\n",
    "\n",
    "    # Fine tune on the target domain dataset\n",
    "    results = model.train(\n",
    "        data=target_data_yaml,         # target domain dataset configuration\n",
    "        epochs=20,                    # typically fewer epochs to adapt to the target domain\n",
    "        imgsz=640,                    # target image size (should match your deployment/inference resolution)\n",
    "        batch=16,                     # batch size\n",
    "        project=\"finetune\",           # project name for saving results\n",
    "        name=\"yolov8n_target_finetune\"  # run name for target domain fine tuning\n",
    "    )\n",
    "\n",
    "    print(\"Target domain fine tuning complete!\")\n",
    "    # Optionally, save the final model checkpoint:\n",
    "    # model.save(\"src/models/yolov8n_target_finetune.pt\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Stage 1: Fine tune on external dataset.\n",
    "    external_model = fine_tune_external()\n",
    "\n",
    "    # Stage 2: Fine tune on target domain (using the best checkpoint from Stage 1).\n",
    "    # You can pass an explicit checkpoint path if you saved it separately.\n",
    "    target_model = fine_tune_target()\n",
    "\n",
    "    print(\"Sequential fine tuning completed successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.nn.tasks import DetectionModel\n",
    "from torch.nn.modules.container import Sequential, ModuleList\n",
    "from ultralytics.nn.modules.conv import Conv, Concat\n",
    "from torch.nn.modules.conv import Conv2d\n",
    "from torch.nn.modules.batchnorm import BatchNorm2d\n",
    "from torch.nn.modules.activation import SiLU\n",
    "from ultralytics.nn.modules.block import C2f, Bottleneck, SPPF, DFL\n",
    "from torch.nn.modules.pooling import MaxPool2d\n",
    "from torch.nn.modules.upsampling import Upsample\n",
    "from ultralytics.nn.modules.head import Detect\n",
    "from ultralytics.utils.loss import v8DetectionLoss, BboxLoss  # Import BboxLoss along with any other loss classes\n",
    "from ultralytics.utils import IterableSimpleNamespace\n",
    "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "from ultralytics.utils.tal import TaskAlignedAssigner\n",
    "\n",
    "# Add all required globals to the safe list\n",
    "torch.serialization.add_safe_globals([\n",
    "    DetectionModel, Sequential, Conv, Conv2d, BatchNorm2d, SiLU, C2f, ModuleList,\n",
    "    Bottleneck, SPPF, MaxPool2d, Upsample, Concat, Detect, DFL,\n",
    "    IterableSimpleNamespace, v8DetectionLoss, BCEWithLogitsLoss, TaskAlignedAssigner, BboxLoss\n",
    "])\n",
    "# Load the best model weights from your training run\n",
    "best_model = YOLO(\"finetune/yolov8n_external_finetune3/weights/best.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "# from ultralytics.nn.tasks import DetectionModel\n",
    "# from torch.nn.modules.container import Sequential, ModuleList\n",
    "# from ultralytics.nn.modules.conv import Conv, Concat\n",
    "# from torch.nn.modules.conv import Conv2d\n",
    "# from torch.nn.modules.batchnorm import BatchNorm2d\n",
    "# from torch.nn.modules.activation import SiLU\n",
    "# from ultralytics.nn.modules.block import C2f, Bottleneck, SPPF, DFL\n",
    "# from torch.nn.modules.pooling import MaxPool2d\n",
    "# from torch.nn.modules.upsampling import Upsample\n",
    "# from ultralytics.nn.modules.head import Detect\n",
    "# from ultralytics.utils.loss import v8DetectionLoss, BboxLoss  # Import BboxLoss along with any other loss classes\n",
    "# from ultralytics.utils import IterableSimpleNamespace\n",
    "# from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "# from ultralytics.utils.tal import TaskAlignedAssigner\n",
    "# from ultralytics.nn.modules.block import C3k2  # adjust the import if needed\n",
    "\n",
    "\n",
    "# # Add all required globals to the safe list\n",
    "# torch.serialization.add_safe_globals([\n",
    "#     DetectionModel, Sequential, Conv, Conv2d, BatchNorm2d, SiLU, C2f, ModuleList,\n",
    "#     Bottleneck, SPPF, MaxPool2d, Upsample, Concat, Detect, DFL,\n",
    "#     IterableSimpleNamespace, v8DetectionLoss, BCEWithLogitsLoss, TaskAlignedAssigner, BboxLoss, C3k2\n",
    "# ])\n",
    "model = YOLO(\"yolo11l.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "people-counter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
