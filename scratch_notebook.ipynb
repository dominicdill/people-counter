{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OpenCV to get frames from url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count People in Web Cam Using YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# HLS playlist URL\n",
    "hls_url = \"https://streamer4.brownrice.com/camdensnowbowl1/camdensnowbowl1.stream/main_playlist.m3u8\"\n",
    "\n",
    "cap = cv2.VideoCapture(hls_url)\n",
    "\n",
    "frame_count = 0\n",
    "frame_skip = 1  # Skip every 5 frames\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the HLS stream.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        frame = frame[600:1050, 2400:3200] # zoom in on the chairlift\n",
    "        if not ret:\n",
    "            print(\"Frame not received, ending stream\")\n",
    "            break\n",
    "        \n",
    "        cv2.imshow(\"HLS Stream\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cv2.imwrite(\"chairlift.jpg\", frame)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# HLS playlist URL\n",
    "hls_url = \"https://streamer4.brownrice.com/camdensnowbowl1/camdensnowbowl1.stream/main_playlist.m3u8\"\n",
    "\n",
    "cap = cv2.VideoCapture(hls_url)\n",
    "\n",
    "# Define polygon coordinates (x, y) as provided\n",
    "polygon_points = np.array([[127, 89], [125, 193], [529, 367], [520, 195]], np.int32)\n",
    "polygon_points = polygon_points.reshape((-1, 1, 2))\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the HLS stream.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Frame not received, ending stream\")\n",
    "            break\n",
    "        \n",
    "        # Crop to zoom in on the chairlift\n",
    "        frame = frame[600:1050, 2400:3200]\n",
    "\n",
    "        # Create a black mask with the same dimensions as the cropped frame (single channel)\n",
    "        mask = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
    "        \n",
    "        # Fill the polygon on the mask with white (255)\n",
    "        cv2.fillPoly(mask, [polygon_points], 255)\n",
    "        \n",
    "        # Apply the mask to the frame; pixels outside the polygon become black\n",
    "        masked_frame = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "        \n",
    "        cv2.imshow(\"HLS Stream\", masked_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cv2.imwrite(\"chairlift.jpg\", masked_frame)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Import the necessary classes - this is a new issue with pytorch 2.6, could use 2.4 and not have to import all of these layers and add them to the globals list\n",
    "from ultralytics.nn.tasks import DetectionModel  # Already imported for YOLOv8 models\n",
    "from torch.nn.modules.container import Sequential    # For Sequential layers\n",
    "from ultralytics.nn.modules.conv import Conv         # For Conv layers defined by Ultralytics\n",
    "from torch.nn.modules.conv import Conv2d              # For PyTorch's Conv2d layer\n",
    "from torch.nn.modules.batchnorm import BatchNorm2d\n",
    "from torch.nn.modules.activation import SiLU               # PyTorch's SiLU activation\n",
    "from ultralytics.nn.modules.block import C2f                       # Ultralytics' C2f block\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from ultralytics.nn.modules.block import Bottleneck\n",
    "from ultralytics.nn.modules.block import SPPF\n",
    "from torch.nn.modules.pooling import MaxPool2d\n",
    "from torch.nn.modules.upsampling import Upsample\n",
    "from ultralytics.nn.modules.conv import Concat\n",
    "from ultralytics.nn.modules.head import Detect\n",
    "from ultralytics.nn.modules.block import DFL\n",
    "torch.serialization.add_safe_globals([\n",
    "    DetectionModel, Sequential, Conv, Conv2d, BatchNorm2d, SiLU, C2f, ModuleList,\n",
    "    Bottleneck, SPPF, MaxPool2d, Upsample, Concat, Detect, DFL\n",
    "])\n",
    "\n",
    "from src.config.config import frame_capture_settings\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the pretrained YOLOv8 model and move it to the appropriate device\n",
    "model = YOLO(os.path.join(frame_capture_settings.model_path, frame_capture_settings.model_name)).to(device)\n",
    "print(model.names)\n",
    "\n",
    "# Define the HLS stream URL (the direct stream URL you extracted)\n",
    "hls_url = \"https://streamer4.brownrice.com/camdensnowbowl1/camdensnowbowl1.stream/main_playlist.m3u8\"\n",
    "\n",
    "# Open the video stream\n",
    "cap = cv2.VideoCapture(hls_url)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the video stream.\")\n",
    "    exit()\n",
    "\n",
    "frame_skip = 20  # Process every 20th frame\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "    frame_count += 1\n",
    "\n",
    "    # Skip frames until we hit the desired interval\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue\n",
    "\n",
    "    # Run YOLOv8 inference on the current frame.\n",
    "    results = model(frame)\n",
    "    \n",
    "    # We'll use a copy of the frame to draw annotations.\n",
    "    annotated_frame = frame.copy()\n",
    "    people_count = 0\n",
    "\n",
    "    # Process each detection result\n",
    "    for result in results:\n",
    "        if result.boxes is not None:\n",
    "            boxes = result.boxes.data.cpu().numpy()  # shape: (num_boxes, 6)\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2, conf, cls = box\n",
    "                # In COCO, the 'person' class typically has an id of 0.\n",
    "                if int(cls) >= 0:\n",
    "                    bbox_color = (0, 255, 0)  # Green\n",
    "                    if cls != 0:\n",
    "                        bbox_color = (255, 0, 0)\n",
    "                    cv2.rectangle(annotated_frame, (int(x1), int(y1)), (int(x2), int(y2)), bbox_color, 2)\n",
    "                    people_count += 1\n",
    "                    # Draw the bounding box and label on the frame.\n",
    "                    cv2.putText(\n",
    "                        annotated_frame,\n",
    "                        f\"{int(cls)} {conf:.2f}\",\n",
    "                        (int(x1), int(y1) - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.5,\n",
    "                        (0, 255, 0),\n",
    "                        2,\n",
    "                    )\n",
    "    \n",
    "    # Overlay the people count on the frame.\n",
    "    cv2.putText(\n",
    "        annotated_frame,\n",
    "        f\"People Count: {people_count}\",\n",
    "        (10, 30),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (0, 0, 255),\n",
    "        2,\n",
    "    )\n",
    "    \n",
    "    # Display the annotated frame\n",
    "    cv2.imshow(\"YOLOv8 People Counting\", annotated_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the stream and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from src.config.config import frame_capture_settings\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the pretrained YOLOv8 model and move it to the appropriate device\n",
    "model1 = YOLO('finetune/yolo11l_webcam_finetune/weights/best.pt').to(device)\n",
    "model2 = YOLO('src/models/yolo11l.pt').to(device)\n",
    "print(model.names)\n",
    "\n",
    "# Define the HLS stream URL (the direct stream URL you extracted)\n",
    "hls_url = \"https://streamer4.brownrice.com/camdensnowbowl1/camdensnowbowl1.stream/main_playlist.m3u8\"\n",
    "\n",
    "# Open the video stream\n",
    "cap = cv2.VideoCapture(hls_url)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the video stream.\")\n",
    "    exit()\n",
    "\n",
    "frame_skip = 2  # Process every 20th frame\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "    frame_count += 1\n",
    "\n",
    "    # Skip frames until we hit the desired interval\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue\n",
    "\n",
    "    # Run YOLOv8 inference on the current frame.\n",
    "    results = model(frame)\n",
    "    \n",
    "    # We'll use a copy of the frame to draw annotations.\n",
    "    annotated_frame = frame.copy()\n",
    "    people_count = 0\n",
    "\n",
    "    # Process each detection result\n",
    "    for result in results:\n",
    "        if result.boxes is not None:\n",
    "            boxes = result.boxes.data.cpu().numpy()  # shape: (num_boxes, 6)\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2, conf, cls = box\n",
    "                # In COCO, the 'person' class typically has an id of 0.\n",
    "                if conf < 0.42:\n",
    "                    continue\n",
    "                if int(cls) >= 0:\n",
    "                    bbox_color = (0, 255, 0)  # Green\n",
    "                    if cls != 0:\n",
    "                        bbox_color = (255, 0, 0)\n",
    "                    cv2.rectangle(annotated_frame, (int(x1), int(y1)), (int(x2), int(y2)), bbox_color, 2)\n",
    "                    people_count += 1\n",
    "                    # Draw the bounding box and label on the frame.\n",
    "                    cv2.putText(\n",
    "                        annotated_frame,\n",
    "                        f\"{int(cls)} {conf:.2f}\",\n",
    "                        (int(x1), int(y1) - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.5,\n",
    "                        (0, 255, 0),\n",
    "                        2,\n",
    "                    )\n",
    "    \n",
    "    # Overlay the people count on the frame.\n",
    "    cv2.putText(\n",
    "        annotated_frame,\n",
    "        f\"People Count: {people_count}\",\n",
    "        (10, 30),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (0, 0, 255),\n",
    "        2,\n",
    "    )\n",
    "    \n",
    "    # Display the annotated frame\n",
    "    cv2.imshow(\"YOLOv8 People Counting\", annotated_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the stream and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset we can use to fine tune the model for counting people on this web cam\n",
    "Saves every 1000th frame into the dataset/images/train folder and the associated YOLOv8 detected people annotations into the dataset/labels/train folder\n",
    "\n",
    "Idea is we then go through these images after we gather a lot and improve upon the annotations. Then we fine tune the YOLO model with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# Set up safe globals for PyTorch 2.6+ (include only if necessary)\n",
    "from ultralytics.nn.tasks import DetectionModel\n",
    "from torch.nn.modules.container import Sequential, ModuleList\n",
    "from ultralytics.nn.modules.conv import Conv, Concat\n",
    "from torch.nn.modules.conv import Conv2d\n",
    "from torch.nn.modules.batchnorm import BatchNorm2d\n",
    "from torch.nn.modules.activation import SiLU\n",
    "from ultralytics.nn.modules.block import C2f, Bottleneck, SPPF, DFL\n",
    "from torch.nn.modules.pooling import MaxPool2d\n",
    "from torch.nn.modules.upsampling import Upsample\n",
    "from ultralytics.nn.modules.head import Detect\n",
    "torch.serialization.add_safe_globals([\n",
    "    DetectionModel, Sequential, Conv, Conv2d, BatchNorm2d, SiLU, C2f, ModuleList,\n",
    "    Bottleneck, SPPF, MaxPool2d, Upsample, Concat, Detect, DFL\n",
    "])\n",
    "\n",
    "# Adjustable parameters\n",
    "WEBCAM_URL = \"https://streamer4.brownrice.com/camdensnowbowl1/camdensnowbowl1.stream/main_playlist.m3u8\"\n",
    "FRAME_INTERVAL = 1000  # Process every 1000th frame\n",
    "CONF_THRESHOLD = 0.4   # Confidence threshold\n",
    "IMG_DIR = 'dataset/images/train'\n",
    "LABEL_DIR = 'dataset/labels/train'\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\").to(device)\n",
    "\n",
    "cap = cv2.VideoCapture(WEBCAM_URL)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the video stream.\")\n",
    "    exit()\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    # Process only every FRAME_INTERVAL frame\n",
    "    if frame_count % FRAME_INTERVAL != 1:\n",
    "        continue\n",
    "\n",
    "    # Save the raw frame\n",
    "    timestamp = int(time.time())\n",
    "    img_filename = os.path.join(IMG_DIR, f\"frame_{frame_count}_{timestamp}.jpg\")\n",
    "    cv2.imwrite(img_filename, frame)\n",
    "    print(f\"Saved image: {img_filename}\")\n",
    "\n",
    "    # Run YOLOv8 inference on the frame\n",
    "    results = model(frame)\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # Open a .txt file for writing the annotations in YOLO format\n",
    "    txt_filename = os.path.splitext(img_filename)[0] + \".txt\"\n",
    "    txt_filename = txt_filename.replace(\"images\", \"labels\")\n",
    "    with open(txt_filename, \"w\") as f:\n",
    "        for result in results:\n",
    "            if result.boxes is not None:\n",
    "                boxes = result.boxes.data.cpu().numpy()  # each row: [x1, y1, x2, y2, conf, cls]\n",
    "                for box in boxes:\n",
    "                    x1, y1, x2, y2, conf, cls = box\n",
    "                    if conf >= CONF_THRESHOLD and int(cls) == 0:  # Only person (class 0)\n",
    "                        # Convert bounding box to YOLO format (normalized)\n",
    "                        x_center = ((x1 + x2) / 2.0) / width\n",
    "                        y_center = ((y1 + y2) / 2.0) / height\n",
    "                        bbox_width = (x2 - x1) / width\n",
    "                        bbox_height = (y2 - y1) / height\n",
    "                        # Write annotation line: class x_center y_center width height\n",
    "                        f.write(f\"0 {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\\n\")\n",
    "    print(f\"Saved annotations: {txt_filename}\")\n",
    "\n",
    "    # Optionally, display the frame (with no annotations drawn)\n",
    "    #cv2.imshow(\"Dataset Collection\", frame)\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hardcoded folder path (change this to your folder)\n",
    "folder_path = 'datasets/dataset_webcam/labels/train'\n",
    "model_path = 'finetune/yolo11l_external_finetune/weights/best.pt'\n",
    "model = YOLO(model_path)\n",
    "\n",
    "def get_yolo_boxes_from_annotation_file(annotation_file):\n",
    "    boxes = []\n",
    "    if not os.path.exists(annotation_file):\n",
    "        print(f\"Annotation file not found: {annotation_file}\")\n",
    "        return boxes  # return empty list if no file\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        # Each line: \"class_id x_center y_center width height\"\n",
    "        box = line.strip().split(' ')\n",
    "        if len(box) != 5:\n",
    "            continue  # skip invalid lines\n",
    "        box = [float(x) for x in box]\n",
    "        boxes.append(box)\n",
    "    return boxes\n",
    "\n",
    "def draw_box(yolo_box, height, width, ax):\n",
    "        \"\"\"Draw a rectangle for a box in YOLO format and return the patch.\"\"\"\n",
    "        xc = yolo_box[1] * width\n",
    "        yc = yolo_box[2] * height\n",
    "        w = yolo_box[3] * width\n",
    "        h = yolo_box[4] * height\n",
    "        x0 = xc - w/2\n",
    "        y0 = yc - h/2\n",
    "        patch = plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2)\n",
    "        ax.add_patch(patch)\n",
    "        \n",
    "\n",
    "# Loop over all items in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    annotation_path = os.path.join(folder_path, filename)\n",
    "    img_filename = filename.replace('.txt', '.jpg')\n",
    "    img = cv2.imread(os.path.join(folder_path, img_filename))\n",
    "    if img is None:\n",
    "        print(f\"Image {img_filename} not found.\")\n",
    "        continue\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    boxes = get_yolo_boxes_from_annotation_file(annotation_path)\n",
    "    if not boxes:\n",
    "        print(f\"No boxes found in {annotation_path}\")\n",
    "        continue\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img)\n",
    "    height, width, _ = img.shape\n",
    "\n",
    "    for box in boxes:\n",
    "        draw_box(box, height, width, ax)\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 25311251\n",
      "Approximate model size (MB): 96.55475997924805\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "model = YOLO('finetune/yolo11l_webcam_finetune/weights/best.pt')\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters:\", total_params)\n",
    "\n",
    "model_size_bytes = total_params * 4  # assuming float32\n",
    "model_size_mb = model_size_bytes / (1024**2)\n",
    "print(\"Approximate model size (MB):\", model_size_mb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 4\n"
     ]
    }
   ],
   "source": [
    "param = next(model.parameters())\n",
    "print(param.dtype, param.element_size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11l summary: 631 layers, 25,311,251 parameters, 0 gradients, 87.3 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(631, 25311251, 0, 87.27372799999999)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514\n"
     ]
    }
   ],
   "source": [
    "x = [p.numel() for p in model.parameters()]\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual in-memory model size (MB): 96.55475997924805\n"
     ]
    }
   ],
   "source": [
    "actual_model_size_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "actual_model_size_mb = actual_model_size_bytes / (1024**2)\n",
    "print(\"Actual in-memory model size (MB):\", actual_model_size_mb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_profiler import memory_usage\n",
    "def run_inference():\n",
    "    # Run a single inference, e.g.:\n",
    "    output = model(input_tensor)\n",
    "    return output\n",
    "\n",
    "mem_usage = memory_usage(proc=run_inference, interval=0.1)\n",
    "print(\"Memory usage (MB):\", max(mem_usage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-22 10:11:52\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "timestamp = 1740237112.471\n",
    "datetime_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))\n",
    "print(datetime_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740236585.140.jpg\t - \t2025-02-22 10:03:05\n",
      "1740236589.066.jpg\t - \t2025-02-22 10:03:09\n",
      "1740236592.881.jpg\t - \t2025-02-22 10:03:12\n",
      "1740236596.312.jpg\t - \t2025-02-22 10:03:16\n",
      "1740236600.234.jpg\t - \t2025-02-22 10:03:20\n",
      "1740236603.762.jpg\t - \t2025-02-22 10:03:23\n",
      "1740236607.111.jpg\t - \t2025-02-22 10:03:27\n",
      "1740236612.281.jpg\t - \t2025-02-22 10:03:32\n",
      "1740236621.283.jpg\t - \t2025-02-22 10:03:41\n",
      "1740236624.909.jpg\t - \t2025-02-22 10:03:44\n",
      "1740236626.989.jpg\t - \t2025-02-22 10:03:46\n",
      "1740236628.670.jpg\t - \t2025-02-22 10:03:48\n",
      "1740236632.240.jpg\t - \t2025-02-22 10:03:52\n",
      "1740236635.815.jpg\t - \t2025-02-22 10:03:55\n",
      "1740236639.568.jpg\t - \t2025-02-22 10:03:59\n",
      "1740236643.253.jpg\t - \t2025-02-22 10:04:03\n",
      "1740236646.711.jpg\t - \t2025-02-22 10:04:06\n",
      "1740236654.017.jpg\t - \t2025-02-22 10:04:14\n",
      "1740236658.052.jpg\t - \t2025-02-22 10:04:18\n",
      "1740236661.702.jpg\t - \t2025-02-22 10:04:21\n",
      "1740236665.286.jpg\t - \t2025-02-22 10:04:25\n",
      "1740236669.513.jpg\t - \t2025-02-22 10:04:29\n",
      "1740236673.810.jpg\t - \t2025-02-22 10:04:33\n",
      "1740236678.566.jpg\t - \t2025-02-22 10:04:38\n",
      "1740236682.278.jpg\t - \t2025-02-22 10:04:42\n",
      "1740236685.866.jpg\t - \t2025-02-22 10:04:45\n",
      "1740236689.632.jpg\t - \t2025-02-22 10:04:49\n",
      "1740236693.328.jpg\t - \t2025-02-22 10:04:53\n",
      "1740236696.985.jpg\t - \t2025-02-22 10:04:56\n",
      "1740236702.924.jpg\t - \t2025-02-22 10:05:02\n",
      "1740237062.429.jpg\t - \t2025-02-22 10:11:02\n",
      "1740237065.450.jpg\t - \t2025-02-22 10:11:05\n",
      "1740237068.280.jpg\t - \t2025-02-22 10:11:08\n",
      "1740237071.291.jpg\t - \t2025-02-22 10:11:11\n",
      "1740237074.429.jpg\t - \t2025-02-22 10:11:14\n",
      "1740237077.364.jpg\t - \t2025-02-22 10:11:17\n",
      "1740237080.207.jpg\t - \t2025-02-22 10:11:20\n",
      "1740237083.151.jpg\t - \t2025-02-22 10:11:23\n",
      "1740237086.090.jpg\t - \t2025-02-22 10:11:26\n",
      "1740237089.055.jpg\t - \t2025-02-22 10:11:29\n",
      "1740237091.993.jpg\t - \t2025-02-22 10:11:31\n",
      "1740237095.002.jpg\t - \t2025-02-22 10:11:35\n",
      "1740237097.882.jpg\t - \t2025-02-22 10:11:37\n",
      "1740237100.783.jpg\t - \t2025-02-22 10:11:40\n",
      "1740237103.577.jpg\t - \t2025-02-22 10:11:43\n",
      "1740237106.587.jpg\t - \t2025-02-22 10:11:46\n",
      "1740237109.506.jpg\t - \t2025-02-22 10:11:49\n",
      "1740237112.471.jpg\t - \t2025-02-22 10:11:52\n",
      "1740237115.340.jpg\t - \t2025-02-22 10:11:55\n",
      "1740237118.222.jpg\t - \t2025-02-22 10:11:58\n",
      "1740237126.020.jpg\t - \t2025-02-22 10:12:06\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for file in sorted(os.listdir(\"/home/ddd/people-counter/demo/raw_frames\")):\n",
    "    print(file, end=\"\\t - \\t\")\n",
    "    timestr = float(file[:-4])\n",
    "    #print(timestr)\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestr)))\n",
    "    # print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(file.split(\".\")[:1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame dimensions: 3648x2052, FPS: 15.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "# HLS playlist URL (or use a webcam by passing 0)\n",
    "hls_url = \"https://streamer4.brownrice.com/camdensnowbowl1/camdensnowbowl1.stream/main_playlist.m3u8\"\n",
    "cap = cv2.VideoCapture(hls_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the stream.\")\n",
    "    exit()\n",
    "\n",
    "# Get frame dimensions and fps from the stream\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "print(f\"Frame dimensions: {frame_width}x{frame_height}, FPS: {fps}\")\n",
    "if fps == 0:  # Fallback if fps isn’t provided by the stream\n",
    "    fps = 25.0\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi', fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Record for a specific duration (e.g., 10 seconds)\n",
    "start_time = time.time()\n",
    "record_duration = int(60*60*0.5)  # seconds\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Frame not received, ending stream\")\n",
    "        break\n",
    "\n",
    "    # Write the frame to the video file\n",
    "    out.write(frame)\n",
    "\n",
    "    # Optionally display the frame\n",
    "    cv2.imshow(\"HLS Stream\", frame)\n",
    "\n",
    "    # Stop after record_duration seconds or if 'q' is pressed\n",
    "    if time.time() - start_time > record_duration or cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# HLS playlist URL\n",
    "\n",
    "cap = cv2.VideoCapture(\"2-26-wednesday-night-30-minutes.avi\")\n",
    "\n",
    "frame_count = 0\n",
    "frame_skip = 15  # Skip every 5 frames\n",
    "background = cv2.imread(\"average_background.jpg\")\n",
    "background = cv2.imread(\"images_for_manual_labeling/triple_chairlift/images/triple_chairlift_camdensnowbowl_2025_02_06_18_59_18_000000.jpg\")\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the HLS stream.\")\n",
    "else:\n",
    "    ret, last_frame = cap.read()\n",
    "    last_frame = last_frame[600:1050, 2400:3200] # zoom in on the chairlift\n",
    "    while True:\n",
    "        frame_count += 1\n",
    "        if frame_count % frame_skip != 0:\n",
    "            continue\n",
    "        ret, frame = cap.read()\n",
    "        last_frame = frame[600:1050, 2400:3200] # zoom in on the chairlift\n",
    "        ret, frame = cap.read()\n",
    "        frame = frame[600:1050, 2400:3200] # zoom in on the chairlift\n",
    "        if not ret:\n",
    "            print(\"Frame not received, ending stream\")\n",
    "            break\n",
    "        diff_frame = cv2.absdiff(last_frame, frame)\n",
    "        #diff_frame = cv2.absdiff()\n",
    "\n",
    "        new_frame = cv2.cvtColor(diff_frame, cv2.COLOR_BGR2GRAY)\n",
    "        new_frame_colored = cv2.cvtColor(new_frame, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        combo_frame = cv2.addWeighted(frame, 1.0, new_frame_colored, 0.5, 0)\n",
    "        last_frame = frame\n",
    "        \n",
    "        #stack the frames horizontally\n",
    "        combo_frame = np.hstack((frame, combo_frame, diff_frame))\n",
    "        # find the difference between \n",
    "\n",
    "        # For debugging, display the frame\n",
    "        cv2.imshow(\"HLS Stream\", combo_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cv2.imwrite(\"chairlift.jpg\", frame)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def load_frame(file_path):\n",
    "    \"\"\"\n",
    "    Load a frame from a file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the image file.\n",
    "        \n",
    "    Returns:\n",
    "        image (numpy.ndarray): Loaded image.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(file_path)\n",
    "    if image is None:\n",
    "        raise ValueError(\"Could not load image. Check path: \" + file_path)\n",
    "    return image\n",
    "\n",
    "def compute_average_image(directory, pattern=\"*.jpg\"):\n",
    "    \"\"\"\n",
    "    Compute the average image from all images in a directory.\n",
    "    \n",
    "    This function loads all images matching the given pattern, accumulates them,\n",
    "    and then computes the per-pixel average. You could also compute the median,\n",
    "    which is often more robust to moving objects.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to the directory containing images.\n",
    "        pattern (str): Glob pattern for image files (default: \"*.jpg\").\n",
    "        \n",
    "    Returns:\n",
    "        average_img (numpy.ndarray): The computed average image.\n",
    "    \"\"\"\n",
    "    files = glob.glob(os.path.join(directory, pattern))\n",
    "    if not files:\n",
    "        raise ValueError(\"No images found in directory using pattern: \" + pattern)\n",
    "    \n",
    "    sum_img = None\n",
    "    count = 0\n",
    "    for file in files:\n",
    "        img = load_frame(file)\n",
    "        img = img.astype(np.float32)\n",
    "        if sum_img is None:\n",
    "            sum_img = np.zeros_like(img)\n",
    "        sum_img += img\n",
    "        count += 1\n",
    "    average_img = sum_img / count\n",
    "    # Convert the result back to 8-bit format.\n",
    "    average_img = cv2.convertScaleAbs(average_img)\n",
    "    return average_img\n",
    "\n",
    "def low_light_preprocessing(image):\n",
    "    \"\"\"\n",
    "    Preprocess a low-light image to enhance details.\n",
    "    \n",
    "    This function applies CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    on the luminance channel after converting the image to the LAB color space.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): Input BGR image.\n",
    "        \n",
    "    Returns:\n",
    "        equalized (numpy.ndarray): Preprocessed image.\n",
    "    \"\"\"\n",
    "    # Convert from BGR to LAB color space.\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    \n",
    "    # Apply CLAHE to the L-channel.\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    \n",
    "    # Merge the CLAHE enhanced L-channel back with a and b.\n",
    "    merged = cv2.merge((cl, a, b))\n",
    "    # Convert back to BGR color space.\n",
    "    equalized = cv2.cvtColor(merged, cv2.COLOR_LAB2BGR)\n",
    "    return equalized\n",
    "\n",
    "def threshold_and_morph_ops(diff_image, threshold_value=30, kernel_size=3):\n",
    "    \"\"\"\n",
    "    Apply thresholding and morphological operations to a difference image.\n",
    "    \n",
    "    Args:\n",
    "        diff_image (numpy.ndarray): Input grayscale difference image.\n",
    "        threshold_value (int): Pixel intensity threshold for binarization.\n",
    "        kernel_size (int): Size of the kernel for morphological operations.\n",
    "        \n",
    "    Returns:\n",
    "        morph (numpy.ndarray): Processed binary mask.\n",
    "    \"\"\"\n",
    "    # Thresholding to obtain a binary image.\n",
    "    _, thresh = cv2.threshold(diff_image, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Create a structuring element.\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    \n",
    "    # Apply morphological opening and closing to remove noise.\n",
    "    morph = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "    morph = cv2.morphologyEx(morph, cv2.MORPH_CLOSE, kernel)\n",
    "    return morph\n",
    "\n",
    "def subtract_background(frame, background):\n",
    "    \"\"\"\n",
    "    Subtract a background image from a frame using absolute difference.\n",
    "    \n",
    "    Args:\n",
    "        frame (numpy.ndarray): Current frame.\n",
    "        background (numpy.ndarray): Pre-computed background (average image).\n",
    "        \n",
    "    Returns:\n",
    "        diff (numpy.ndarray): Difference image.\n",
    "    \"\"\"\n",
    "    diff = cv2.absdiff(frame, background)\n",
    "    return diff\n",
    "\n",
    "def display_frame(window_name, frame, delay=0):\n",
    "    \"\"\"\n",
    "    Display a frame in a window.\n",
    "    \n",
    "    Args:\n",
    "        window_name (str): The title of the display window.\n",
    "        frame (numpy.ndarray): The image/frame to display.\n",
    "        delay (int): Delay in milliseconds for cv2.waitKey.\n",
    "    \"\"\"\n",
    "    cv2.imshow(window_name, frame)\n",
    "    cv2.waitKey(delay)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the directory containing the images.\n",
    "    image_directory = \"images_for_manual_labeling/triple_chairlift/images\"\n",
    "    \n",
    "    # Compute the average background image if it doesn't already exist.\n",
    "    if not os.path.exists(\"average_background .jpg\"):\n",
    "        background = compute_average_image(image_directory, pattern=\"*.jpg\")\n",
    "        cv2.imwrite(\"average_background.jpg\", background)\n",
    "    \n",
    "    # Load a sample frame (modify the file name as needed).\n",
    "    sample_file = \"images_for_manual_labeling/triple_chairlift/images/triple_chairlift_camdensnowbowl_2025_02_06_17_36_00_000000.jpg\"\n",
    "    frame = load_frame(sample_file)\n",
    "    \n",
    "    # Optionally preprocess the frame if it's low light.\n",
    "    preprocessed_frame = low_light_preprocessing(frame)\n",
    "    \n",
    "    # Subtract the background to highlight moving objects.\n",
    "    diff = subtract_background(preprocessed_frame, background)\n",
    "    \n",
    "    # Convert the difference image to grayscale.\n",
    "    gray_diff = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply thresholding and morphological operations.\n",
    "    processed_mask = threshold_and_morph_ops(gray_diff, threshold_value=30, kernel_size=3)\n",
    "    \n",
    "    # Display the results.\n",
    "    display_frame(\"Original Frame\", frame, delay=1000)\n",
    "    display_frame(\"Preprocessed Frame\", preprocessed_frame, delay=1000)\n",
    "    display_frame(\"Difference Image\", gray_diff, delay=1000)\n",
    "    display_frame(\"Processed Mask\", processed_mask, delay=0)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        cap.release()\n",
    "    \n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def load_frame(file_path):\n",
    "    \"\"\"\n",
    "    Load an image from the given file path.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the JPEG image.\n",
    "    \n",
    "    Returns:\n",
    "        image (numpy.ndarray): Loaded image.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(file_path)\n",
    "    if image is None:\n",
    "        raise ValueError(\"Could not load image: \" + file_path)\n",
    "    return image\n",
    "\n",
    "def create_median_background(folder_path, num_files=25, blur_kernel=(5, 5)):\n",
    "    \"\"\"\n",
    "    Create a median background image from a random sample of JPEG files in a folder.\n",
    "    Each image is blurred before computing the median.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing JPEG images.\n",
    "        num_files (int): Number of random images to select for creating the background.\n",
    "        blur_kernel (tuple): Kernel size for Gaussian blur.\n",
    "    \n",
    "    Returns:\n",
    "        median_frame (numpy.ndarray): The computed median background image.\n",
    "    \"\"\"\n",
    "    # Get list of all JPEG files in the folder.\n",
    "    files = glob.glob(os.path.join(folder_path, '*.jpg'))\n",
    "    \n",
    "    if len(files) < num_files:\n",
    "        raise ValueError(f\"Not enough images in folder to select {num_files} images.\")\n",
    "    \n",
    "    # Randomly select num_files without replacement.\n",
    "    selected_files = np.random.choice(files, num_files, replace=False)\n",
    "    \n",
    "    frames = []\n",
    "    for file in selected_files:\n",
    "        frame = load_frame(file)\n",
    "        # Apply a Gaussian blur to reduce noise\n",
    "        frame = cv2.GaussianBlur(frame, blur_kernel, 0)\n",
    "        frames.append(frame)\n",
    "    \n",
    "    # Compute the median along the time axis.\n",
    "    median_frame = np.median(np.array(frames), axis=0).astype(np.uint8)\n",
    "    return median_frame\n",
    "\n",
    "def display_frame(window_name, frame, delay=0):\n",
    "    \"\"\"\n",
    "    Display a frame using OpenCV.\n",
    "    \n",
    "    Args:\n",
    "        window_name (str): The name of the window.\n",
    "        frame (numpy.ndarray): The image to display.\n",
    "        delay (int): Delay in milliseconds for cv2.waitKey.\n",
    "    \"\"\"\n",
    "    cv2.imshow(window_name, frame)\n",
    "    cv2.waitKey(delay)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the path to your images folder.\n",
    "    folder_path = \"images_for_manual_labeling/triple_chairlift/images\"  # Change this to your actual folder path.\n",
    "    \n",
    "    # Create the median background using 25 random JPEG images.\n",
    "    median_background = create_median_background(folder_path, num_files=25)\n",
    "    \n",
    "    # Display the median background.\n",
    "    display_frame(\"Median Background\", median_background, delay=0)\n",
    "    \n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camdensnowbowl_2025_03_06_17_16_40.txt\n",
      "camdensnowbowl_2025_03_06_17_13_03.txt\n",
      "camdensnowbowl_2025_03_06_17_12_12.txt\n",
      "camdensnowbowl_2025_03_06_17_15_55.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for f in os.listdir(\"/home/ddd/people-counter/images_for_manual_labeling/labels/model_default\"):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "people-counter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
