{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OpenCV to get frames from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# HLS playlist URL\n",
    "hls_url = \"https://streamer4.brownrice.com/camdensnowbowl1/camdensnowbowl1.stream/main_playlist.m3u8\"\n",
    "\n",
    "cap = cv2.VideoCapture(hls_url)\n",
    "\n",
    "frame_count = 0\n",
    "frame_skip = 1  # Skip every 5 frames\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the HLS stream.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        #frame = frame[600:1050, 2400:3200] # zoom in on the chairlift\n",
    "        if not ret:\n",
    "            print(\"Frame not received, ending stream\")\n",
    "            break\n",
    "        # frame_count += 1\n",
    "        # if frame_count % frame_skip != 0:\n",
    "        #     continue\n",
    "        # Process your frame (e.g., people counting) here\n",
    "\n",
    "        # For debugging, display the frame\n",
    "        cv2.imshow(\"HLS Stream\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count People in Web Cam Using YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Import the necessary classes - this is a new issue with pytorch 2.6, could use 2.4 and not have to import all of these layers and add them to the globals list\n",
    "from ultralytics.nn.tasks import DetectionModel  # Already imported for YOLOv8 models\n",
    "from torch.nn.modules.container import Sequential    # For Sequential layers\n",
    "from ultralytics.nn.modules.conv import Conv         # For Conv layers defined by Ultralytics\n",
    "from torch.nn.modules.conv import Conv2d              # For PyTorch's Conv2d layer\n",
    "from torch.nn.modules.batchnorm import BatchNorm2d\n",
    "from torch.nn.modules.activation import SiLU               # PyTorch's SiLU activation\n",
    "from ultralytics.nn.modules.block import C2f                       # Ultralytics' C2f block\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from ultralytics.nn.modules.block import Bottleneck\n",
    "from ultralytics.nn.modules.block import SPPF\n",
    "from torch.nn.modules.pooling import MaxPool2d\n",
    "from torch.nn.modules.upsampling import Upsample\n",
    "from ultralytics.nn.modules.conv import Concat\n",
    "from ultralytics.nn.modules.head import Detect\n",
    "from ultralytics.nn.modules.block import DFL\n",
    "torch.serialization.add_safe_globals([\n",
    "    DetectionModel, Sequential, Conv, Conv2d, BatchNorm2d, SiLU, C2f, ModuleList,\n",
    "    Bottleneck, SPPF, MaxPool2d, Upsample, Concat, Detect, DFL\n",
    "])\n",
    "\n",
    "from src.config.config import frame_capture_settings\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the pretrained YOLOv8 model and move it to the appropriate device\n",
    "model = YOLO(os.path.join(frame_capture_settings.model_path, frame_capture_settings.model_name)).to(device)\n",
    "print(model.names)\n",
    "\n",
    "# Define the HLS stream URL (the direct stream URL you extracted)\n",
    "hls_url = \"https://streamer4.brownrice.com/camdensnowbowl1/camdensnowbowl1.stream/main_playlist.m3u8\"\n",
    "\n",
    "# Open the video stream\n",
    "cap = cv2.VideoCapture(hls_url)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the video stream.\")\n",
    "    exit()\n",
    "\n",
    "frame_skip = 20  # Process every 20th frame\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "    frame_count += 1\n",
    "\n",
    "    # Skip frames until we hit the desired interval\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue\n",
    "\n",
    "    # Run YOLOv8 inference on the current frame.\n",
    "    results = model(frame)\n",
    "    \n",
    "    # We'll use a copy of the frame to draw annotations.\n",
    "    annotated_frame = frame.copy()\n",
    "    people_count = 0\n",
    "\n",
    "    # Process each detection result\n",
    "    for result in results:\n",
    "        if result.boxes is not None:\n",
    "            boxes = result.boxes.data.cpu().numpy()  # shape: (num_boxes, 6)\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2, conf, cls = box\n",
    "                # In COCO, the 'person' class typically has an id of 0.\n",
    "                if int(cls) >= 0:\n",
    "                    bbox_color = (0, 255, 0)  # Green\n",
    "                    if cls != 0:\n",
    "                        bbox_color = (255, 0, 0)\n",
    "                    cv2.rectangle(annotated_frame, (int(x1), int(y1)), (int(x2), int(y2)), bbox_color, 2)\n",
    "                    people_count += 1\n",
    "                    # Draw the bounding box and label on the frame.\n",
    "                    cv2.putText(\n",
    "                        annotated_frame,\n",
    "                        f\"{int(cls)} {conf:.2f}\",\n",
    "                        (int(x1), int(y1) - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.5,\n",
    "                        (0, 255, 0),\n",
    "                        2,\n",
    "                    )\n",
    "    \n",
    "    # Overlay the people count on the frame.\n",
    "    cv2.putText(\n",
    "        annotated_frame,\n",
    "        f\"People Count: {people_count}\",\n",
    "        (10, 30),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (0, 0, 255),\n",
    "        2,\n",
    "    )\n",
    "    \n",
    "    # Display the annotated frame\n",
    "    cv2.imshow(\"YOLOv8 People Counting\", annotated_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the stream and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from src.config.config import frame_capture_settings\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the pretrained YOLOv8 model and move it to the appropriate device\n",
    "model1 = YOLO('finetune/yolo11l_webcam_finetune/weights/best.pt').to(device)\n",
    "model2 = YOLO('src/models/yolo11l.pt').to(device)\n",
    "print(model.names)\n",
    "\n",
    "# Define the HLS stream URL (the direct stream URL you extracted)\n",
    "hls_url = \"https://streamer4.brownrice.com/camdensnowbowl1/camdensnowbowl1.stream/main_playlist.m3u8\"\n",
    "\n",
    "# Open the video stream\n",
    "cap = cv2.VideoCapture(hls_url)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the video stream.\")\n",
    "    exit()\n",
    "\n",
    "frame_skip = 2  # Process every 20th frame\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "    frame_count += 1\n",
    "\n",
    "    # Skip frames until we hit the desired interval\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue\n",
    "\n",
    "    # Run YOLOv8 inference on the current frame.\n",
    "    results = model(frame)\n",
    "    \n",
    "    # We'll use a copy of the frame to draw annotations.\n",
    "    annotated_frame = frame.copy()\n",
    "    people_count = 0\n",
    "\n",
    "    # Process each detection result\n",
    "    for result in results:\n",
    "        if result.boxes is not None:\n",
    "            boxes = result.boxes.data.cpu().numpy()  # shape: (num_boxes, 6)\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2, conf, cls = box\n",
    "                # In COCO, the 'person' class typically has an id of 0.\n",
    "                if conf < 0.42:\n",
    "                    continue\n",
    "                if int(cls) >= 0:\n",
    "                    bbox_color = (0, 255, 0)  # Green\n",
    "                    if cls != 0:\n",
    "                        bbox_color = (255, 0, 0)\n",
    "                    cv2.rectangle(annotated_frame, (int(x1), int(y1)), (int(x2), int(y2)), bbox_color, 2)\n",
    "                    people_count += 1\n",
    "                    # Draw the bounding box and label on the frame.\n",
    "                    cv2.putText(\n",
    "                        annotated_frame,\n",
    "                        f\"{int(cls)} {conf:.2f}\",\n",
    "                        (int(x1), int(y1) - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.5,\n",
    "                        (0, 255, 0),\n",
    "                        2,\n",
    "                    )\n",
    "    \n",
    "    # Overlay the people count on the frame.\n",
    "    cv2.putText(\n",
    "        annotated_frame,\n",
    "        f\"People Count: {people_count}\",\n",
    "        (10, 30),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (0, 0, 255),\n",
    "        2,\n",
    "    )\n",
    "    \n",
    "    # Display the annotated frame\n",
    "    cv2.imshow(\"YOLOv8 People Counting\", annotated_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the stream and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset we can use to fine tune the model for counting people on this web cam\n",
    "Saves every 1000th frame into the dataset/images/train folder and the associated YOLOv8 detected people annotations into the dataset/labels/train folder\n",
    "\n",
    "Idea is we then go through these images after we gather a lot and improve upon the annotations. Then we fine tune the YOLO model with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# Set up safe globals for PyTorch 2.6+ (include only if necessary)\n",
    "from ultralytics.nn.tasks import DetectionModel\n",
    "from torch.nn.modules.container import Sequential, ModuleList\n",
    "from ultralytics.nn.modules.conv import Conv, Concat\n",
    "from torch.nn.modules.conv import Conv2d\n",
    "from torch.nn.modules.batchnorm import BatchNorm2d\n",
    "from torch.nn.modules.activation import SiLU\n",
    "from ultralytics.nn.modules.block import C2f, Bottleneck, SPPF, DFL\n",
    "from torch.nn.modules.pooling import MaxPool2d\n",
    "from torch.nn.modules.upsampling import Upsample\n",
    "from ultralytics.nn.modules.head import Detect\n",
    "torch.serialization.add_safe_globals([\n",
    "    DetectionModel, Sequential, Conv, Conv2d, BatchNorm2d, SiLU, C2f, ModuleList,\n",
    "    Bottleneck, SPPF, MaxPool2d, Upsample, Concat, Detect, DFL\n",
    "])\n",
    "\n",
    "# Adjustable parameters\n",
    "WEBCAM_URL = \"https://streamer4.brownrice.com/camdensnowbowl1/camdensnowbowl1.stream/main_playlist.m3u8\"\n",
    "FRAME_INTERVAL = 1000  # Process every 1000th frame\n",
    "CONF_THRESHOLD = 0.4   # Confidence threshold\n",
    "IMG_DIR = 'dataset/images/train'\n",
    "LABEL_DIR = 'dataset/labels/train'\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\").to(device)\n",
    "\n",
    "cap = cv2.VideoCapture(WEBCAM_URL)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the video stream.\")\n",
    "    exit()\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    # Process only every FRAME_INTERVAL frame\n",
    "    if frame_count % FRAME_INTERVAL != 1:\n",
    "        continue\n",
    "\n",
    "    # Save the raw frame\n",
    "    timestamp = int(time.time())\n",
    "    img_filename = os.path.join(IMG_DIR, f\"frame_{frame_count}_{timestamp}.jpg\")\n",
    "    cv2.imwrite(img_filename, frame)\n",
    "    print(f\"Saved image: {img_filename}\")\n",
    "\n",
    "    # Run YOLOv8 inference on the frame\n",
    "    results = model(frame)\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # Open a .txt file for writing the annotations in YOLO format\n",
    "    txt_filename = os.path.splitext(img_filename)[0] + \".txt\"\n",
    "    txt_filename = txt_filename.replace(\"images\", \"labels\")\n",
    "    with open(txt_filename, \"w\") as f:\n",
    "        for result in results:\n",
    "            if result.boxes is not None:\n",
    "                boxes = result.boxes.data.cpu().numpy()  # each row: [x1, y1, x2, y2, conf, cls]\n",
    "                for box in boxes:\n",
    "                    x1, y1, x2, y2, conf, cls = box\n",
    "                    if conf >= CONF_THRESHOLD and int(cls) == 0:  # Only person (class 0)\n",
    "                        # Convert bounding box to YOLO format (normalized)\n",
    "                        x_center = ((x1 + x2) / 2.0) / width\n",
    "                        y_center = ((y1 + y2) / 2.0) / height\n",
    "                        bbox_width = (x2 - x1) / width\n",
    "                        bbox_height = (y2 - y1) / height\n",
    "                        # Write annotation line: class x_center y_center width height\n",
    "                        f.write(f\"0 {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\\n\")\n",
    "    print(f\"Saved annotations: {txt_filename}\")\n",
    "\n",
    "    # Optionally, display the frame (with no annotations drawn)\n",
    "    #cv2.imshow(\"Dataset Collection\", frame)\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hardcoded folder path (change this to your folder)\n",
    "folder_path = 'datasets/dataset_webcam/labels/train'\n",
    "model_path = 'finetune/yolo11l_external_finetune/weights/best.pt'\n",
    "model = YOLO(model_path)\n",
    "\n",
    "def get_yolo_boxes_from_annotation_file(annotation_file):\n",
    "    boxes = []\n",
    "    if not os.path.exists(annotation_file):\n",
    "        print(f\"Annotation file not found: {annotation_file}\")\n",
    "        return boxes  # return empty list if no file\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        # Each line: \"class_id x_center y_center width height\"\n",
    "        box = line.strip().split(' ')\n",
    "        if len(box) != 5:\n",
    "            continue  # skip invalid lines\n",
    "        box = [float(x) for x in box]\n",
    "        boxes.append(box)\n",
    "    return boxes\n",
    "\n",
    "def draw_box(yolo_box, height, width, ax):\n",
    "        \"\"\"Draw a rectangle for a box in YOLO format and return the patch.\"\"\"\n",
    "        xc = yolo_box[1] * width\n",
    "        yc = yolo_box[2] * height\n",
    "        w = yolo_box[3] * width\n",
    "        h = yolo_box[4] * height\n",
    "        x0 = xc - w/2\n",
    "        y0 = yc - h/2\n",
    "        patch = plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2)\n",
    "        ax.add_patch(patch)\n",
    "        \n",
    "\n",
    "# Loop over all items in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    annotation_path = os.path.join(folder_path, filename)\n",
    "    img_filename = filename.replace('.txt', '.jpg')\n",
    "    img = cv2.imread(os.path.join(folder_path, img_filename))\n",
    "    if img is None:\n",
    "        print(f\"Image {img_filename} not found.\")\n",
    "        continue\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    boxes = get_yolo_boxes_from_annotation_file(annotation_path)\n",
    "    if not boxes:\n",
    "        print(f\"No boxes found in {annotation_path}\")\n",
    "        continue\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img)\n",
    "    height, width, _ = img.shape\n",
    "\n",
    "    for box in boxes:\n",
    "        draw_box(box, height, width, ax)\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 25311251\n",
      "Approximate model size (MB): 96.55475997924805\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "model = YOLO('finetune/yolo11l_webcam_finetune/weights/best.pt')\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters:\", total_params)\n",
    "\n",
    "model_size_bytes = total_params * 4  # assuming float32\n",
    "model_size_mb = model_size_bytes / (1024**2)\n",
    "print(\"Approximate model size (MB):\", model_size_mb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 4\n"
     ]
    }
   ],
   "source": [
    "param = next(model.parameters())\n",
    "print(param.dtype, param.element_size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11l summary: 631 layers, 25,311,251 parameters, 0 gradients, 87.3 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(631, 25311251, 0, 87.27372799999999)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514\n"
     ]
    }
   ],
   "source": [
    "x = [p.numel() for p in model.parameters()]\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual in-memory model size (MB): 96.55475997924805\n"
     ]
    }
   ],
   "source": [
    "actual_model_size_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "actual_model_size_mb = actual_model_size_bytes / (1024**2)\n",
    "print(\"Actual in-memory model size (MB):\", actual_model_size_mb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_profiler import memory_usage\n",
    "def run_inference():\n",
    "    # Run a single inference, e.g.:\n",
    "    output = model(input_tensor)\n",
    "    return output\n",
    "\n",
    "mem_usage = memory_usage(proc=run_inference, interval=0.1)\n",
    "print(\"Memory usage (MB):\", max(mem_usage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-22 10:11:52\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "timestamp = 1740237112.471\n",
    "datetime_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))\n",
    "print(datetime_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740236585.140.jpg\t - \t2025-02-22 10:03:05\n",
      "1740236589.066.jpg\t - \t2025-02-22 10:03:09\n",
      "1740236592.881.jpg\t - \t2025-02-22 10:03:12\n",
      "1740236596.312.jpg\t - \t2025-02-22 10:03:16\n",
      "1740236600.234.jpg\t - \t2025-02-22 10:03:20\n",
      "1740236603.762.jpg\t - \t2025-02-22 10:03:23\n",
      "1740236607.111.jpg\t - \t2025-02-22 10:03:27\n",
      "1740236612.281.jpg\t - \t2025-02-22 10:03:32\n",
      "1740236621.283.jpg\t - \t2025-02-22 10:03:41\n",
      "1740236624.909.jpg\t - \t2025-02-22 10:03:44\n",
      "1740236626.989.jpg\t - \t2025-02-22 10:03:46\n",
      "1740236628.670.jpg\t - \t2025-02-22 10:03:48\n",
      "1740236632.240.jpg\t - \t2025-02-22 10:03:52\n",
      "1740236635.815.jpg\t - \t2025-02-22 10:03:55\n",
      "1740236639.568.jpg\t - \t2025-02-22 10:03:59\n",
      "1740236643.253.jpg\t - \t2025-02-22 10:04:03\n",
      "1740236646.711.jpg\t - \t2025-02-22 10:04:06\n",
      "1740236654.017.jpg\t - \t2025-02-22 10:04:14\n",
      "1740236658.052.jpg\t - \t2025-02-22 10:04:18\n",
      "1740236661.702.jpg\t - \t2025-02-22 10:04:21\n",
      "1740236665.286.jpg\t - \t2025-02-22 10:04:25\n",
      "1740236669.513.jpg\t - \t2025-02-22 10:04:29\n",
      "1740236673.810.jpg\t - \t2025-02-22 10:04:33\n",
      "1740236678.566.jpg\t - \t2025-02-22 10:04:38\n",
      "1740236682.278.jpg\t - \t2025-02-22 10:04:42\n",
      "1740236685.866.jpg\t - \t2025-02-22 10:04:45\n",
      "1740236689.632.jpg\t - \t2025-02-22 10:04:49\n",
      "1740236693.328.jpg\t - \t2025-02-22 10:04:53\n",
      "1740236696.985.jpg\t - \t2025-02-22 10:04:56\n",
      "1740236702.924.jpg\t - \t2025-02-22 10:05:02\n",
      "1740237062.429.jpg\t - \t2025-02-22 10:11:02\n",
      "1740237065.450.jpg\t - \t2025-02-22 10:11:05\n",
      "1740237068.280.jpg\t - \t2025-02-22 10:11:08\n",
      "1740237071.291.jpg\t - \t2025-02-22 10:11:11\n",
      "1740237074.429.jpg\t - \t2025-02-22 10:11:14\n",
      "1740237077.364.jpg\t - \t2025-02-22 10:11:17\n",
      "1740237080.207.jpg\t - \t2025-02-22 10:11:20\n",
      "1740237083.151.jpg\t - \t2025-02-22 10:11:23\n",
      "1740237086.090.jpg\t - \t2025-02-22 10:11:26\n",
      "1740237089.055.jpg\t - \t2025-02-22 10:11:29\n",
      "1740237091.993.jpg\t - \t2025-02-22 10:11:31\n",
      "1740237095.002.jpg\t - \t2025-02-22 10:11:35\n",
      "1740237097.882.jpg\t - \t2025-02-22 10:11:37\n",
      "1740237100.783.jpg\t - \t2025-02-22 10:11:40\n",
      "1740237103.577.jpg\t - \t2025-02-22 10:11:43\n",
      "1740237106.587.jpg\t - \t2025-02-22 10:11:46\n",
      "1740237109.506.jpg\t - \t2025-02-22 10:11:49\n",
      "1740237112.471.jpg\t - \t2025-02-22 10:11:52\n",
      "1740237115.340.jpg\t - \t2025-02-22 10:11:55\n",
      "1740237118.222.jpg\t - \t2025-02-22 10:11:58\n",
      "1740237126.020.jpg\t - \t2025-02-22 10:12:06\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for file in sorted(os.listdir(\"/home/ddd/people-counter/demo/raw_frames\")):\n",
    "    print(file, end=\"\\t - \\t\")\n",
    "    timestr = float(file[:-4])\n",
    "    #print(timestr)\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestr)))\n",
    "    # print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(file.split(\".\")[:1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "people-counter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
